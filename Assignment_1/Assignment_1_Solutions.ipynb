{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_data = [[0,3,-1]]\n",
    "test_result = [[0,3,0]]\n",
    "\n",
    "def check(function):\n",
    "    return function(test_data[0])==test_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is a category of machine learning that uses artificial neural networks to construct regressions/classifiers etc.. The field is called \"deep\" for two reasons.\n",
    "\n",
    "1. Representation. Rather than relying on hand engineered featueres the networks are allowed to learn some function mapping the data from it's original manifold to some other manifold that the classifier/regression output is better able to perform predictions on. The technique is deep in that it allows for a construction of some deeper more useful represntation rather than learning on the data in its native structure.\n",
    "\n",
    "2. Architecture. Neural Networks learn this mapping through application of many layers of functions. Unlike the first neural networks which might contain only a single perceptron modern Neural Network architectures can encode billions of parameters over many thousands of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Name three factors that lead to the explosive growth of Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Widespread high quality datasets available on the Web\n",
    "2. Cheap GPU compute\n",
    "3. Highly available market for ML uses due to evolving internet infrastructure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What does the No Free Lunch Theorem State?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The No Free Lunch theorem essentially states that there is no perfect machine learning solution. \n",
    "\n",
    "\"We have dubbed the associated results NFL theorems because they demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems.\" - Wolport & Macready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What can we say about $\\sum_{i=0}^nf_i$ if all $f$ are linear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear combination of linear functions is linear. Stacking linear nodes in a network no matter how deep will only yield linear results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Given a nonlinear function $f$ we wish to approximate using a linear model $g$ what could technique could we apply so that $g$ is capable of learning $f$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we need some nonlinear function $v$ so that $g(v(x))$ does a good job at acting like $f(x)$\n",
    "\n",
    "1. We could do something like what is done in the kernel trick of SVM and let $g$ be infinite dimension. This doesen't actually work very well lol.\n",
    "2. We could hand engineer some $g$. This was the approach taken for quite some time but is brittle and constrained by number of trained experts to do the work. Does not generalize at all.\n",
    "3. We use a neural network and allow the networks hidden layers to learn some nonlinear function $g$ that maps the input of $f$ into some space where a linear classifer/regression is capable of learning it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What are the implications of learning with a nonconvex loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning with a convex loss function implies that if I discover a minimum I have discovered the global minimum. A Nonconvex loss function can make no such claims. I have to worry about being trapped in bad local minima and need some kind of criteria to tell me when the training I've done is good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What is a ReLU function? Why are they popular? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU stands for Rectified Linear Unit. ReLU functions are used as nonlinear functions in hidden layers, they are attractive for being computationaly trivial while being capable of injecting a nonlinearity into the hidden layers allowing the network to learn non linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Code a ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(vector):\n",
    "    output = np.maximum(0,vector)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What does the Universal Approximation Theorem State, what are the implications for Deep Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Universal Approximation Theorem says that any network of large enough size with a squashing function is capable of learning any Borel measurable function from one finite dimensional space to another. This means that for any continuous function there is a neural network that encodes it, it actually is a little deeper than that since there are Borel functions which are non-continous. This is an interesting theoretical result however the Universal Approximation Theorem makes no claims about the feasibility of learning said network for a given function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What was the most interesting thing you learned this week?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discussion of the AI winter was an interesting refresher. It's cool to see how a handful of scientists sticking to their guns had such an incredible impact on the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
